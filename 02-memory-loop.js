// 02-memory-loop.js
// ç›®æ ‡ï¼šæ‰‹åŠ¨å®ç°â€œè®°å¿†â€ (Memory)
// åŸç†ï¼šæŠŠä¹‹å‰çš„å¯¹è¯ (History) å­˜å…¥æ•°ç»„ï¼Œæ¯æ¬¡è¯·æ±‚æ—¶å…¨éƒ¨å‘é€ã€‚

import OpenAI from 'openai';
import readline from 'readline';
import dotenv from 'dotenv';
dotenv.config();

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// ğŸ§  è¿™é‡Œçš„æ•°ç»„å°±æ˜¯ Agent çš„â€œçŸ­æœŸè®°å¿†â€ (Context Window)
const history = [
  { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªåå« Jarvis çš„ AI åŠ©æ‰‹ã€‚ä½ è¯´è¯å¹½é»˜é£è¶£ã€‚' } // System Prompt: è®¾å®šäººè®¾
];

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

console.log("ğŸ¤– Jarvis åœ¨çº¿ã€‚è¾“å…¥ 'exit' é€€å‡ºã€‚");

function ask() {
  rl.question('\nUser: ', async (input) => {
    if (input.toLowerCase() === 'exit') {
      rl.close();
      return;
    }

    // 1. æŠŠç”¨æˆ·çš„è¾“å…¥åŠ å…¥è®°å¿†
    history.push({ role: 'user', content: input });

    try {
      // 2. å‘é€ *æ•´ä¸ª* history ç»™ LLM
      const completion = await client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: history, // <--- å…³é”®ç‚¹ï¼šå‘é€æ‰€æœ‰å†å²
      });

      const reply = completion.choices[0].message.content;
      
      // 3. æŠŠ AI çš„å›ç­”ä¹ŸåŠ å…¥è®°å¿†
      history.push({ role: 'assistant', content: reply });

      console.log(`Jarvis: ${reply}`);
      
      // æ˜¾ç¤ºå½“å‰çš„ Token æ¶ˆè€— (æ¨¡æ‹Ÿ)
      console.log(`(å½“å‰ Context é•¿åº¦: ${history.length} æ¡æ¶ˆæ¯)`);

    } catch (error) {
      console.error("Error:", error.message);
    }

    ask(); // ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯
  });
}

ask();
